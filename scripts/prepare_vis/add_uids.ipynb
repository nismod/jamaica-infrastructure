{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8daf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import pathlib\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import geopandas\n",
    "import numpy\n",
    "import pandas\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac56d55",
   "metadata": {},
   "source": [
    "# Add UIDs to networks data\n",
    "\n",
    "Read each network file, add `uid` column with unique integer id, for consistent reference in database and MBTiles\n",
    "for visualisation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6d0ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    \"\"\"Read config.json\"\"\"\n",
    "    config_path = os.path.join(os.path.dirname(__file__), \"..\", \"..\", \"config.json\")\n",
    "    with open(config_path, \"r\") as config_fh:\n",
    "        config = json.load(config_fh)\n",
    "    return config\n",
    "\n",
    "base_path = load_config()['paths']['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe82c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = pandas.read_csv(f\"{base_path}/processed_data/networks/network_layers_hazard_intersections_details.csv\")\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a138b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ids = []\n",
    "counts = []\n",
    "vis_fnames = []\n",
    "for i, layer in enumerate(layers.itertuples()):\n",
    "    print(layer.sector, layer.asset_gpkg, layer.asset_layer)\n",
    "    \n",
    "    base_id = i * 100_000_000\n",
    "    base_ids.append(base_id)\n",
    "    \n",
    "    layer_data = geopandas.read_file(os.path.join(\"..\", layer.path), layer=layer.asset_layer)\n",
    "    count =  len(layer_data)\n",
    "    counts.append(count)\n",
    "    layer_data['uid'] = numpy.arange(base_id, base_id + count)\n",
    "    \n",
    "    out_fname = os.path.join(base_path, 'processed_data', layer.path.replace(\"networks\", \"networks_uids\"))\n",
    "    if \"buildings\" in out_fname:\n",
    "        out_fname = out_fname.replace(\"buildings/\", \"networks_uids/buildings/\")\n",
    "    pathlib.Path(os.path.dirname(out_fname)).mkdir(parents=True, exist_ok=True)\n",
    "    vis_fnames.append(out_fname)\n",
    "    \n",
    "    layer_data.to_file(\n",
    "        out_fname, \n",
    "        layer=layer.asset_layer, \n",
    "        index=False,\n",
    "        driver='GPKG')\n",
    "\n",
    "layers['base_id'] = base_ids\n",
    "layers['count'] = counts\n",
    "layers['vis_path'] = vis_fnames\n",
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers.to_csv(f\"{base_path}/processed_data/networks_uids/network_details.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c58a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del layer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2295ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8183f278",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Process `direct_damages_summary` results into parquet files with integer UIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e3a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = pandas.read_csv(f\"{base_path}/processed_data/networks_uids/network_details.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50167cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ['damages.parquet','exposures.parquet','losses.parquet', 'EAD_EAEL.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c67eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_fname(layer):\n",
    "    return f\"{base_path}/processed_data/networks_uids/{layer.asset_gpkg}_{layer.asset_layer}_ids.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902c6571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_fname(layer, suffix, clean=False):\n",
    "    if clean:\n",
    "        return f\"{base_path}/results/direct_damages_summary_uids/{layer.asset_gpkg}_{layer.asset_layer}_{suffix}\"\n",
    "    else:\n",
    "        return f\"{base_path}/processed_data/results/direct_damages_summary/{layer.asset_gpkg}_{layer.asset_layer}_{suffix}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92591259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in layers.itertuples():\n",
    "    layer_data = geopandas.read_file(layer.vis_path, layer=layer.asset_layer)\n",
    "    id_lookup = layer_data[[layer.asset_id_column, 'uid']]\n",
    "    id_lookup.to_parquet(get_id_fname(layer), index=False)\n",
    "    print(get_id_fname(layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d3f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_layer(layer):\n",
    "    id_lookup = pandas.read_parquet(get_id_fname(layer)).set_index(layer.asset_id_column)\n",
    "    for suffix in suffixes:\n",
    "        try:\n",
    "            print(get_results_fname(layer, suffix, clean=False))\n",
    "            if 'parquet' in suffix:\n",
    "                data = pandas.read_parquet(get_results_fname(layer, suffix, clean=False))\n",
    "            elif 'csv' in suffix:\n",
    "                data = pandas.read_csv(get_results_fname(layer, suffix, clean=False), dtype={'rcp':object})\n",
    "            else:\n",
    "                print(f\"WARN Skipping suffix with unhandled filetype: {suffix}\")\n",
    "                continue\n",
    "            linked = data.set_index(layer.asset_id_column).join(id_lookup).reset_index()\n",
    "            assert len(data) == len(linked),  (len(data),len(linked))\n",
    "\n",
    "            linked.to_parquet(get_results_fname(layer, suffix.replace('csv', 'parquet'), clean=True))\n",
    "        except FileNotFoundError as ex:\n",
    "            print(ex)\n",
    "            \n",
    "for layer in layers.itertuples():\n",
    "    if 'buildings' in layer.asset_gpkg:\n",
    "        continue\n",
    "    process_layer(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd3064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hazards = ['coastal', 'cyclone', 'fluvial', 'surface']\n",
    "rcps = ['rcp_2.6', 'rcp_4.5', 'rcp_8.5', 'rcp_baseline']\n",
    "epochs = [\n",
    "    'epoch_2010',\n",
    "    'epoch_2030',\n",
    "    'epoch_2050',\n",
    "    'epoch_2070',\n",
    "    'epoch_2080',\n",
    "    'epoch_2100',\n",
    "]\n",
    "\n",
    "def process_buildings(layer):\n",
    "    id_lookup = pandas.read_parquet(get_id_fname(layer)).set_index(layer.asset_id_column)\n",
    "    for suffix in suffixes:\n",
    "        try:\n",
    "            fname = get_results_fname(layer, suffix, clean=False)\n",
    "            print(fname)\n",
    "            if 'parquet' in suffix:\n",
    "                pf = pq.ParquetFile(fname)\n",
    "                for hazard, rcp, epoch in itertools.product(hazards, rcps, epochs):\n",
    "                    base_cols = ['osm_id'] + [col for col in pf.schema.names if 'unit' in col]\n",
    "                    data_cols = [col for col in pf.schema.names if hazard in col and rcp in col and epoch in col]\n",
    "                    if data_cols:\n",
    "                        print(base_cols, hazard, rcp, epoch, len(data_cols))\n",
    "                        process_subset(layer, fname, base_cols, data_cols, id_lookup, hazard, rcp, epoch, suffix)\n",
    "                \n",
    "            elif 'csv' in suffix:\n",
    "                data = pandas.read_csv(get_results_fname(layer, suffix, clean=False), dtype={'rcp':object})\n",
    "                linked = data.set_index(layer.asset_id_column).join(id_lookup).reset_index()\n",
    "                linked.to_parquet(get_results_fname(layer, suffix.replace('csv', 'parquet'), clean=True))\n",
    "        except Exception as ex:\n",
    "            raise ex\n",
    "\n",
    "def process_subset(layer, fname, base_cols, data_cols, id_lookup, hazard, rcp, epoch, suffix):\n",
    "    data = pandas.read_parquet(\n",
    "        fname,\n",
    "        columns=base_cols+data_cols\n",
    "    )\n",
    "    linked = data.set_index(layer.asset_id_column).join(id_lookup).reset_index()\n",
    "    linked.to_parquet(get_results_fname(layer, f\"{hazard}__{rcp}__{epoch}__{suffix}\", clean=True))\n",
    "\n",
    "for layer in layers.itertuples():\n",
    "    if 'buildings' in layer.asset_gpkg:\n",
    "        process_buildings(layer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
